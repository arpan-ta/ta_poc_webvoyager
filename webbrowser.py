# -*- coding: utf-8 -*-
"""webbrowser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cmN2K-3mx3MAg3qavbUkCq8aIKwMKWQ0
"""

# !pip install -U --quiet langgraph langsmith langchain_openai
#
# !pip install --upgrade --quiet  playwright
# !playwright install

#

import os
from getpass import getpass

from langchain_core.prompts.chat import ChatPromptTemplate


def _getpass(env_var: str):
    if not os.environ.get(env_var):
        os.environ[env_var] = getpass(f"{env_var}=")


_getpass("OPENAI_API_KEY")#

import nest_asyncio
import tempfile
# This is just required for running async playwright in a Jupyter notebook
nest_asyncio.apply()

from typing import List, Optional
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage, SystemMessage
from playwright.async_api import Page


class BBox(TypedDict):
    x: float
    y: float
    text: str
    type: str
    ariaLabel: str


class Prediction(TypedDict):
    action: str
    args: Optional[List[str]]


# This represents the state of the agent
# as it proceeds through execution
class AgentState(TypedDict):
    page: Page  # The Playwright web page lets us interact with the web environment
    input: str  # User request
    img: str  # b64 encoded screenshot
    bboxes: List[BBox]  # The bounding boxes from the browser annotation function
    prediction: Prediction  # The Agent's output
    # A system message (or messages) containing the intermediate steps
    scratchpad: List[BaseMessage]
    observation: str  # The most recent response from a tool

"""# AGENT TOOLS

1.   Click (at labeled box)
2.   Type
3.   Scroll
4.    Wait
5. Go back
6. Go to seach engine (google)



"""

import asyncio
import platform
# from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
# async def detect_captchatype(state: AgentState):
from langgraph.types import Command, interrupt

# def detect_captcha(state):
#     """Detects CAPTCHA presence and type on the webpage using LLM."""
#
#     page_html = state.get("page", None)
#     captcha_image_url = state.get("captcha_image_url", None)
#
#     if not page_html and not captcha_image_url:
#         return {**state, "captcha_type": None, "captcha_status": "No CAPTCHA detected"}
#
#     messages = [SystemMessage(content="You are an AI that detects CAPTCHA types.")]
#
#     if captcha_image_url:
#         messages.append(HumanMessage(content=f"Analyze this CAPTCHA image: {captcha_image_url}"))
#     elif page_html:
#         messages.append(HumanMessage(content=f"Analyze this webpage HTML and detect if a CAPTCHA exists: {page_html}"))
#
#     response = chat(messages)
#
#     return {**state, "captcha_type": response.content.strip(), "captcha_status": "CAPTCHA detected"}




async def click(state: AgentState):
    # - Click [Numerical_Label]
    import random
    wait = random.randint(1, 10)
    # time.sleep(wait)
    page = state["page"]
    click_args = state["prediction"]["args"]
    if click_args is None or len(click_args) != 1:
        return f"Failed to click bounding box labeled as number {click_args}"
    bbox_id = click_args[0]
    bbox_id = int(bbox_id)
    try:
        bbox = state["bboxes"][bbox_id]
    except Exception:
        return f"Error: no bbox for : {bbox_id}"
    x, y = bbox["x"], bbox["y"]
    await page.mouse.click(x, y)
    # TODO: In the paper, they automatically parse any downloaded PDFs
    # We could add something similar here as well and generally
    # improve response format.
    return f"Clicked {bbox_id}"


async def type_text(state: AgentState):
    try:
        page = state["page"]
        type_args = state["prediction"]["args"]
        if type_args is None or len(type_args) != 2:
            return (
                f"Failed to type in element from bounding box labeled as number {type_args}"
            )
        bbox_id = type_args[0]
        bbox_id = int(bbox_id)
        bbox = state["bboxes"][bbox_id]
        x, y = bbox["x"], bbox["y"]
        text_content = type_args[1]
        await page.mouse.click(x, y)
        # Check if MacOS
        select_all = "Meta+A" if platform.system() == "Darwin" else "Control+A"
        await page.keyboard.press(select_all)
        await page.keyboard.press("Backspace")
        await page.keyboard.type(text_content)
        # await page.keyboard.press("Enter")
        return f"Typed {text_content} and submitted"
    except ValueError as e:
        print(e)


async def scroll(state: AgentState):
    page = state["page"]
    scroll_args = state["prediction"]["args"]
    if scroll_args is None or len(scroll_args) != 2:
        return "Failed to scroll due to incorrect arguments."

    target, direction = scroll_args

    if target.upper() == "WINDOW":
        # Not sure the best value for this:
        scroll_amount = 500
        scroll_direction = (
            -scroll_amount if direction.lower() == "up" else scroll_amount
        )
        await page.evaluate(f"window.scrollBy(0, {scroll_direction})")
    else:
        # Scrolling within a specific element
        scroll_amount = 200
        target_id = int(target)
        bbox = state["bboxes"][target_id]
        x, y = bbox["x"], bbox["y"]
        scroll_direction = (
            -scroll_amount if direction.lower() == "up" else scroll_amount
        )
        await page.mouse.move(x, y)
        await page.mouse.wheel(0, scroll_direction)

    return f"Scrolled {direction} in {'window' if target.upper() == 'WINDOW' else 'element'}"


def human_feedback(state: AgentState):
    print("---human_feedback---")
    feedback = interrupt("Please provide feedback:")
    return {"user_feedback": feedback}

async def wait(state: AgentState):
    sleep_time = 5
    await asyncio.sleep(sleep_time)
    return f"Waited for {sleep_time}s."


async def go_back(state: AgentState):
    page = state["page"]
    await page.go_back()
    return f"Navigated back a page to {page.url}."

async def to_google(state: AgentState):
    page = state["page"]
    await page.goto("https://www.google.com/")
    return "Navigated to google.com."

"""# template for Agent state which gets updated using chain of thought"""

from typing import List, Optional
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage, SystemMessage
from playwright.async_api import Page


class BBox(TypedDict):
    x: float
    y: float
    text: str
    type: str
    ariaLabel: str


class Prediction(TypedDict):
    action: str
    args: Optional[List[str]]


# This represents the state of the agent
# as it proceeds through execution
class AgentState(TypedDict):
    page: Page  # The Playwright web page lets us interact with the web environment
    input: str  # User request
    img: str  # b64 encoded screenshot
    bboxes: List[BBox]  # The bounding boxes from the browser annotation function
    prediction: Prediction  # The Agent's output
    # A system message (or messages) containing the intermediate steps
    scratchpad: List[BaseMessage]
    observation: str  # The most recent response from a tool

"""# additional tool to update scratch pad - and state"""

import re


def update_scratchpad(state: AgentState):
    """After a tool is invoked, we want to update
    the scratchpad so the agent is aware of its previous steps"""
    old = state.get("scratchpad")
    if old:
        txt = old[0].content
        last_line = txt.rsplit("\n", 1)[-1]
        step = int(re.match(r"\d+", last_line).group()) + 1
    else:
        txt = "Previous action observations:\n"
        step = 1
    txt += f"\n{step}. {state['observation']}"

    return {**state, "scratchpad": [SystemMessage(content=txt)]}


async def summarize_image(state: AgentState):
    """Summarizes an image with a short context of what is displayed."""
    from io import BytesIO
    page = state["page"]
    screenshot = await page.screenshot()
    image_url = base64.b64encode(screenshot)
    if not image_url:
        return {**state, "observation": "No image provided"}
    print('invoke summarize image')
    # Load Image
    image_data = base64.b64decode(image_url)#.decode("utf-8")
    # img = Image.open(BytesIO(image_data))
    img = Image.open(BytesIO(image_data))

    # Create a temporary file
    # # with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as temp_file:
    #     img.save(temp_file, format="JPEG")
    #     image_data = temp_file.name
    file = 'image.jpg'
    img.save(file)

    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode("utf-8")

    base64_image = encode_image(file)

    # Send image to GPT-4o for summarization
    messages = [
        SystemMessage(content="You are an AI that summarizes images concisely."),
        HumanMessage(content="Describe and summarize this image in a super short, meaningful way."),
        HumanMessage(content= [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}},
                            ])]
    # https: // platform.openai.com / docs / guides / vision
    #https://github.com/langchain-ai/langchain/discussions/23374
    # response = llm(messages)
    # print(response.content)
    # Store the summary in the agent state
    chat_prompt_template = ChatPromptTemplate.from_messages(
        messages=messages)

    from langchain_core.output_parsers import StrOutputParser
    output_parser = StrOutputParser()
    chain = chat_prompt_template | llm | output_parser
    result = chain.invoke({})
    print(result)
    return f"{result}"#, "observation": response.content}

async def request_user_input(state: AgentState):
    """
    Requests user input when required.
    """
    print("\n--- Awaiting User Input ---")
    user_input = input("Enter the required input: ")  # Get user input

    # Store the input in state["prediction"]["args"]

    return {**state, "observation": f"User provided input: {user_input}"}
"""# screen shot page"""

import base64

from langchain_core.runnables import chain as chain_decorator

# Some javascript we will run on each step
# to take a screenshot of the page, select the
# elements to annotate, and add bounding boxes
with open("mark_page.js") as f:
    mark_page_script = f.read()


@chain_decorator
async def mark_page(page):
    await page.evaluate(mark_page_script)
    for _ in range(10):
        try:
            bboxes = await page.evaluate("markPage()")
            break
        except Exception:
            # May be loading...
            asyncio.sleep(3)
    screenshot = await page.screenshot()
    # Ensure the bboxes don't follow us around
    await page.evaluate("unmarkPage()")
    return {
        "img": base64.b64encode(screenshot).decode(),
        "bboxes": bboxes,
    }



from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI


async def annotate(state):
    marked_page = await mark_page.with_retry().ainvoke(state["page"])
    return {**state, **marked_page}


def format_descriptions(state):
    labels = []
    for i, bbox in enumerate(state["bboxes"]):
        text = bbox.get("ariaLabel") or ""
        if not text.strip():
            text = bbox["text"]
        el_type = bbox.get("type")
        labels.append(f'{i} (<{el_type}/>): "{text}"')
    bbox_descriptions = "\nValid Bounding Boxes:\n" + "\n".join(labels)
    return {**state, "bbox_descriptions": bbox_descriptions}


def parse(text: str) -> dict:
    action_prefix = "Action: "
    if not text.strip().split("\n")[-1].startswith(action_prefix):
        return {"action": "retry", "args": f"Could not parse LLM Output: {text}"}
    action_block = text.strip().split("\n")[-1]

    action_str = action_block[len(action_prefix) :]
    split_output = action_str.split(" ", 1)
    if len(split_output) == 1:
        action, action_input = split_output[0], None
    else:
        action, action_input = split_output
    action = action.strip()
    if action_input is not None:
        action_input = [
            inp.strip().strip("[]") for inp in action_input.strip().split(";")
        ]
    return {"action": action, "args": action_input}



"""PROMPT HANDLING AND PARSING"""

SYSTEM_PROMPT = """
SYSTEM
Imagine you are a robot browsing the web, just like humans. Now you need to complete a task.
In each iteration, you will receive an Observation that includes a screenshot of a webpage and some texts.
This screenshot will feature Numerical Labels placed in the TOP LEFT corner of each Web Element.
Carefully analyze the visual to detect any CAPTCHA that erquires user inputs, otherwise continue to analyze the visual
information to identify the Numerical Label corresponding to the Web Element that requires interaction, then follow
the guidelines and choose one of the following actions:

1. Click a Web Element.
2. Delete existing content in a textbox and then type content.
3. Scroll up or down.
4. Wait
5. Go back
7. Return to google to start over.
8. Respond with the final answer
9. request for user input 
10. summarize text in an image

Correspondingly, Action should STRICTLY follow the format:

- Click [Numerical_Label]
- Type [Numerical_Label]; [Content]
- Scroll [Numerical_Label or WINDOW]; [up or down]
- user_input [Numerical_Label]; [content]
- Wait
- GoBack
- Google
- summarize_image
- ANSWER [content]

Key Guidelines You MUST follow:

* Action guidelines *
1) Execute only one action per iteration.
2) When clicking or typing, ensure to select the correct bounding box.
3) Numeric labels lie in the top-left corner of their corresponding bounding boxes and are colored the same.
4) seek user input when user has to provide any detail or values to fill in
5) While asking for user input, verify the number of digits needed for the required code and map to correct numerical labels to improve action Type

* Web Browsing Guidelines *
1) Select strategically to minimize time wasted.
2) If user asks to loging with email-id, it is okay to help user to login
3) When you encounter a CAPTCHA, your Action must be "summarize_image" and then "Wait" from above listed actions, and should strictly follow this
4) If you are able to login or complete task, action must be "ANSWER"


Your reply should strictly follow the format:

Thought: {{Your brief thoughts (briefly summarize the info that will help ANSWER)}}
Action: {{One Action format you choose}}
Then the User will provide:
Observation: {{A labeled screenshot Given by User}}"""

prompt = hub.pull("wfh/web-voyager")
# print(prompt.messages[0].prompt[0].template)
prompt.messages[0].prompt[0].template = SYSTEM_PROMPT
updated_prompt = prompt
# print(updated_prompt.messages[0].prompt[0].template)

llm = ChatOpenAI(model="gpt-4o", max_tokens=4096)
agent = annotate | RunnablePassthrough.assign(
    prediction=format_descriptions | updated_prompt | llm | StrOutputParser() | parse
)

"""# lang graph build"""

from langchain_core.runnables import RunnableLambda

from langgraph.graph import END, START, StateGraph

graph_builder = StateGraph(AgentState)
from langchain_core.runnables.graph_png import PngDrawer

graph_builder.add_node("agent", agent)
graph_builder.add_edge(START, "agent")
graph_builder.add_node("update_scratchpad", update_scratchpad)
graph_builder.add_edge("update_scratchpad", "agent")
# graph_builder.add_node('user_input',)
tools = {
    "Click": click,
    "Type": type_text,
    "Scroll": scroll,
    "Wait": wait,
    "GoBack": go_back,
    "Google": to_google,
"user_input": request_user_input,
"summarize_image": summarize_image,

}


for node_name, tool in tools.items():
    graph_builder.add_node(
        node_name,
        # The lambda ensures the function's string output is mapped to the "observation"
        # key in the AgentState
        RunnableLambda(tool) | (lambda observation: {"observation": observation}),
    )
    # Always return to the agent (by means of the update-scratchpad node)
    graph_builder.add_edge(node_name, "update_scratchpad")
    # graph_builder.add_edge(node_name, "summarize_image")

def select_tool(state: AgentState):
    # Any time the agent completes, this function
    # is called to route the output to a tool or
    # to the end user.
    action = state["prediction"]["action"]
    print('select tool:', action)
    # if action == "ANSWER":
    #     return END
    if action == "retry":
        return "agent"
    return action

graph_builder.add_node(
        'ANSWER',
        # The lambda ensures the function's string output is mapped to the "observation"
        # key in the AgentState
        RunnableLambda(summarize_image) | (lambda observation: {"observation": observation}),
    )
graph_builder.add_conditional_edges("agent", select_tool)
graph_builder.add_edge("ANSWER", END)
graph = graph_builder.compile()
from IPython.display import Image, display


try:
    PNG_GRAPH = graph.get_graph().draw_mermaid_png()

    with open("my_graph.png", "wb") as f:
        f.write(PNG_GRAPH)
    display(Image(PNG_GRAPH))


except Exception:
    # This requires some extra dependencies and is optional
    pass

"""# graph stream"""

from IPython import display
from playwright.async_api import async_playwright
from PIL import Image
browser = asyncio.run(async_playwright().start())
# We will set headless=False so we can watch the agent navigate the web.
browser = asyncio.run(browser.chromium.launch(headless=False, args=None))
page = asyncio.run(browser.new_page())
_ = asyncio.run(page.goto("https://www.booking.com"))

async def call_agent(question: str, page, max_steps: int = 150):
    event_stream = graph.astream(
        {
            "page": page,
            "input": question,
            "scratchpad": [],
        },
        {
            "recursion_limit": max_steps,
        },
    )
    final_answer = None
    steps = []
    async for event in event_stream:

        # We'll display an event stream here
        if "agent" not in event:
            continue
        pred = event["agent"].get("prediction") or {}
        pred
        action = pred.get("action")
        action_input = pred.get("args")
        display.clear_output(wait=False)
        steps.append(f"{len(steps) + 1}. {action}: {action_input}")
        # print('observation: ', event['agent'].get('observation', 'no observation'))
        print('next action:::::')
        print(f"{len(steps) + 1}. {action}: {action_input}")

        # display.display(display.Image(base64.b64decode(event["agent"]["img"])))
        # if "ANSWER" in action:
        #     final_answer = action_input[0]
        #     break

    return final_answer



"""# playground"""

res = asyncio.run(call_agent(" Log into my account using praneeji93@gmail.com ?", page))
print(f"Final response: {res}")

